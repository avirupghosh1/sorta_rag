# Hog RAGger

## Overview
This project implements a RAG pipeline that leverages state-of-the-art models and databases for efficient document retrieval and question answering. The pipeline utilizes Hugging Face Transformers, Chroma for vector storage, and FAISS for similarity search.

## Tech Stack

- **Programming Language:** Python
- **Frameworks:** Hugging Face Transformers for NLP tasks
- **Databases:** 
  - Chroma for vector storage
  - FAISS for efficient similarity search

## Approach

1. **Data Storage:**
   - The initial step involves storing the corpus data along with its associated metadata from `corpus.json` provided in the uploaded files.

2. **Question Classification:**
   - We classify incoming questions into predefined categories using the model **MoritzLaurer/deberta-v3-large-zeroshot-v2.0**. 
   - The classification results serve as filters for the subsequent vector similarity search.

3. **Document Chunking:**
   - Retrieved documents are processed for chunking using the model **BlueOrangeDigital/distilbert-cross-segment-document-chunking**. 
   - This approach is adapted from [Medium Article on Document Chunking](https://medium.com/blue-orange-digital/harnessing-transformers-to-chunk-documents-exploring-an-open-source-model-for-context-aware-e5775c017cd4).

4. **Embedding Generation:**
   - The chunks are then converted into embeddings and stored in a FAISS database to facilitate efficient similarity search.

5. **Enhanced Similarity Search:**
   - Similarity search is performed on the embeddings to improve the quality of the retrieved results.

6. **Final Question Classification:**
   - The refined results are classified again using both **MoritzLaurer/deberta-v3-large-zeroshot-v2.0** and **facebook/bart-large-mnli** to determine the nature of the queries: inference, comparison, or temporal.

7. **Question Answering:**
   - Based on the classification, the model **meta-llama/Llama-3.2-1B-Instruct** is used to generate answers.
   - If the model outputs insufficient information, the query is classified as `null_query`.

8. **Flowchart:**

   - Overall flow of the document can be looked up on the [flowchart](https://github.com/AranitheOracle/SOTA_RAG/blob/main/flow.pdf) uploaded.

## Other Approaches Explored

1. **Self-Query Retriever System**: We attempted to build a self-query retrieval system by leveraging metadata filters for more accurate retrieval. Resources referenced included:
   - [How to Build a RAG System with a Self-Querying Retriever in LangChain](https://towardsdatascience.com/how-to-build-a-rag-system-with-a-self-querying-retriever-in-langchain-16b4fa23e9ad)
   - [LangChain Documentation: Self-Query Retriever](https://python.langchain.com/docs/how_to/self_query)

   - Despite our efforts, we faced challenges with the practical implementation of metadata-based filters in the retrieval pipeline.

2. **Determining Confidence Level in LLM Answers**: To improve answer reliability and minimize hallucination, we explored approaches to evaluate the confidence level of answers generated by LLMs by using a feedback loop. We referred to:
   - [A Confidence Score for LLM Answers](https://medium.com/wbaa/a-confidence-score-for-llm-answers-c668844d52c8)


## Getting Started

### Prerequisites

- Python installed on your machine
- Google Colab access

### Setup Instructions

1. **Download the Project:**
   - Download the `.ipynb` [file](https://github.com/AranitheOracle/SOTA_RAG/blob/main/rag_sota.ipynb) from the repository.

2. **Upload to Google Colab:**
   - Go to [Google Colab](https://colab.research.google.com/).
   - Upload the downloaded `.ipynb` file.

3. **Prepare Data:**
   - Upload the `vector_store.zip` file provided in the repository to the Colab environment.

4. **Run the Notebook:**
   - Execute the cells in the notebook to run the pipeline.

## Conclusion

This project showcases an innovative approach to document retrieval and question answering using cutting-edge NLP models and techniques. The integration of vector databases ensures efficiency and accuracy in handling user queries.

## Acknowledgments

- [Medium Article on Document Chunking](https://medium.com/blue-orange-digital/harnessing-transformers-to-chunk-documents-exploring-an-open-source-model-for-context-aware-e5775c017cd4).

- 
