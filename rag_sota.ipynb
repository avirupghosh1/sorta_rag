{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 430,
      "metadata": {
        "id": "WgSKKDSYgxK_"
      },
      "outputs": [],
      "source": [
        "# Who is the individual associated with the cryptocurrency industry facing a criminal trial on fraud and conspiracy charges, as reported by both The Verge and TechCrunch, and is accused by prosecutors of committing fraud for personal gain?\n",
        "\n",
        "# After the TechCrunch report on October 7, 2023, concerning Dave Clark's comments on Flexport, and the subsequent TechCrunch article on October 30, 2023, regarding Ryan Petersen's actions at Flexport, was there a change in the nature of the events reported?\n",
        "# Do the TechCrunch article on software companies and the Hacker News article on The Epoch Times both report an increase in revenue related to payment and subscription models, respectively?\n",
        "\n",
        "# Considering the information from a BBC article detailing Sridevi's achievements in the Indian film industry and a Times of India report on her posthumous honors, which single character from a film portrayed by Sridevi has been recognized for its cultural impact and has also been commemorated with a special award after her passing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "id": "_D0Y7FiSWDWm"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {
        "id": "uucMD9Ii9rbi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# import json\n",
        "# from langchain_core.documents import Document\n",
        "\n",
        "# # Load JSON data\n",
        "# def load_data(filepath: str):\n",
        "#     with open(filepath, 'r') as f:\n",
        "#         data = json.load(f)\n",
        "#     return data\n",
        "\n",
        "# # Prepare documents from JSON for embedding and indexing\n",
        "# def preprocess_documents(data):\n",
        "#     documents = []\n",
        "#     for entry in data:\n",
        "#         # Create a Document object with page content and metadata\n",
        "#         documents.append(\n",
        "#             Document(\n",
        "#                 page_content=entry['body'],\n",
        "#                 metadata={\n",
        "#                     'title': entry['title'] if entry.get('title') else \"none\",\n",
        "#                     'author': entry['author'] if entry.get('author') else \"none\",\n",
        "#                     'source': entry['source'] if entry.get('source') else \"none\",\n",
        "#                     'published_at': entry['published_at'] if entry.get('published_at') else \"none\",\n",
        "#                     'category': entry['category'] if entry.get('category') else \"none\",\n",
        "#                     'url': entry['url'] if entry.get('url') else \"none\",\n",
        "#                 }\n",
        "#             )\n",
        "#         )\n",
        "#     return documents\n",
        "\n",
        "# # Load and process the corpus.json file\n",
        "# data = load_data('/content/corpus.json')\n",
        "# docs = preprocess_documents(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 433,
      "metadata": {
        "id": "ndoqxI-WWCgU"
      },
      "outputs": [],
      "source": [
        "# from langchain_chroma import Chroma\n",
        "# from langchain_core.documents import Document\n",
        "# from langchain.embeddings import GPT4AllEmbeddings  # Import GPT4All embeddings\n",
        "# # gpt4all_embd = GPT4AllEmbeddings()\n",
        "# vectorstore = Chroma.from_documents(docs, GPT4AllEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {
        "id": "TqqmS44I7d2u"
      },
      "outputs": [],
      "source": [
        "\n",
        "import zipfile\n",
        "\n",
        "# Unzip the uploaded file\n",
        "with zipfile.ZipFile(\"/content/vector_store.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/chroma_vectorstore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {
        "collapsed": true,
        "id": "lMvvkIfuirK1"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import GPT4AllEmbeddings\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "\n",
        "# Path to the existing Chroma database and vector files\n",
        "persist_directory = '/content/chroma_vectorstore'  # e.g., 'vector' folder\n",
        "\n",
        "# Load the existing Chroma vector store (from chroma.sqlite3 and the directory)\n",
        "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=GPT4AllEmbeddings())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aYY3sINzQApx",
        "outputId": "a8db6834-f028-4c66-e254-07e176a89abc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the zero-shot classification pipeline with the specified model\n",
        "pipe = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")\n",
        "\n",
        "# Define the available categories\n",
        "available_categories = [\"business\", \"entertainment\", \"health\", \"science\", \"sports\", \"technology\"]\n",
        "\n",
        "# Function to classify the query using the zero-shot classification pipeline\n",
        "def classify_query(query: str):\n",
        "    # Use the pipeline to classify the query\n",
        "    result = pipe(query, available_categories)\n",
        "\n",
        "    # Extract the top 2 labels and their scores\n",
        "    top_categories = result['labels'][:2]  # Get the top 2 categories\n",
        "    return top_categories\n",
        "\n",
        "# Prepare a function that takes in a query and returns a structured query with categories and filter\n",
        "def process_query(query: str) -> dict:\n",
        "    # Classify the query using zero-shot classification\n",
        "    categories = classify_query(query)\n",
        "\n",
        "    # Construct the filter based on the top 2 categories returned\n",
        "    if categories:\n",
        "        # Create a filter using only eq comparators for the top 2 categories\n",
        "        filter_expression = ' or '.join([f'eq(\"category\", \"{category}\")' for category in categories])\n",
        "        filter_expression = f'({filter_expression})'\n",
        "\n",
        "        # Create the output\n",
        "        output = {\n",
        "            \"query\": query,\n",
        "            \"categories\": categories,\n",
        "            \"filter\": filter_expression\n",
        "        }\n",
        "    else:\n",
        "        # If there's an issue, fall back to return only the query\n",
        "        output = {\"query\": query, \"filter\": \"\"}\n",
        "\n",
        "    return output\n",
        "# query not in train :: Considering the information from an article by The Verge on the latest iCloud security features and a report by Bloomberg on the recent iCloud service outages, which character from the CEO's name of the company responsible for iCloud would be common to both the security feature's codename and the name of the city where the most significant server disruption occurred?\n",
        "# Example usage\n",
        "query=\"Which individual is implicated in both inflating the value of a Manhattan apartment to a figure not yet achieved in New York City's real estate history, according to 'Fortune', and is also accused of adjusting this apartment's valuation to compensate for a loss in another asset's worth, as reported by 'The Age'?\"\n",
        "result = process_query(query)\n",
        "# print(result)\n",
        "\n",
        "# Display the structured query with inferred categories and filter\n",
        "r = result['query'] + result['filter']\n",
        "# print(r)\n",
        "\n",
        "\n",
        "d = vectorstore.similarity_search(r, k=3)\n",
        "print(len(d))\n",
        "docu = \"\"\n",
        "for i in d:\n",
        "    docu += i.page_content\n",
        "# print(docu)\n",
        "label=\"\"\n",
        "if(len(d)==0):\n",
        "    label=\"null_query\"\n",
        "    # output = {\n",
        "    # \"query\": query,\n",
        "    # \"answer\": \"insufficient_information\",\n",
        "    # \"question_type\": \"null_query\",\n",
        "    # \"evidence_list\":[]\n",
        "    # }\n",
        "print(label)\n",
        "\n",
        "    # Convert to JSON string\n",
        "    # json_output = json.dumps(output, indent=4)\n",
        "    # result = generate_json_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3G6pSbeIWBN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 446,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6haReZ60gec",
        "outputId": "a5b18a7d-36a9-4f15-c7d4-0ccd705c869c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "temporal_query\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the zero-shot classification pipeline with the specified model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")\n",
        "# Define the possible labels for the first classification\n",
        "candidate_labels = [\"temporal_query\", \"inference_query\", \"comparison_query\"]\n",
        "\n",
        "# Classify the query using the first model (DeBERTa)\n",
        "result = classifier(query, candidate_labels)\n",
        "\n",
        "# Output the label with the highest score\n",
        "first_label = result['labels'][0]  # Access the first label\n",
        "\n",
        "if first_label == \"inference_query\":\n",
        "    label = \"inference_query\"\n",
        "else:\n",
        "    # If it's not \"inference_query\", classify using the second model (RoBERTa)\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "    # Define the possible labels for the second classification\n",
        "    candidate_labels = [\"temporal_query\", \"comparison_query\"]\n",
        "\n",
        "    # Classify the query again\n",
        "    result = classifier(query, candidate_labels)\n",
        "\n",
        "    # Output the label with the highest score\n",
        "    label = result['labels'][0]\n",
        "\n",
        "# Print the final label\n",
        "print(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 447,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yRIjNQVxV22r",
        "outputId": "6a693f24-80d6-4ec8-d281-1c39d7194849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cut\n",
            "cut\n",
            "cut\n",
            "cut\n",
            "cut\n",
            "cut\n",
            "cut\n",
            "cut\n",
            "cut\n",
            "cut\n",
            "Document split into 25 chunks.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import DistilBertTokenizer\n",
        "if (label!=\"null_query\"):\n",
        "    ordered_sentences = docu.split('\\n')[:-1]\n",
        "    model_name = \"BlueOrangeDigital/distilbert-cross-segment-document-chunking\"\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def right_truncate_sentence(sentence, tokenizer, max_len):\n",
        "      tokenized = tokenizer.encode(sentence)[1:-1]\n",
        "      if len(tokenized) > max_len:\n",
        "          print(\"cut\")\n",
        "      return tokenizer.decode(tokenized[:max_len])\n",
        "\n",
        "\n",
        "    def left_truncate_sentence(sentence, tokenizer, max_len):\n",
        "      tokenized = tokenizer.encode(sentence)[1:-1]\n",
        "      if len(tokenized) > max_len:\n",
        "          print(\"cut\")\n",
        "      return tokenizer.decode(tokenized[-max_len:])\n",
        "\n",
        "    def bucket_pair(left_sentence, right_sentence, tokenizer, max_len):\n",
        "      return left_truncate_sentence(left_sentence, tokenizer, max_len) + \" [SEP] \" + \\\n",
        "          right_truncate_sentence(right_sentence, tokenizer, max_len)\n",
        "\n",
        "    MAX_LEN = 255\n",
        "    pairs = [\n",
        "      bucket_pair(ordered_sentences[i], ordered_sentences[i+1], tokenizer, MAX_LEN)\n",
        "      for i in range(0, len(ordered_sentences) - 1)\n",
        "    ]\n",
        "\n",
        "    from transformers import (\n",
        "      AutoModelForSequenceClassification,\n",
        "      TextClassificationPipeline\n",
        "    )\n",
        "\n",
        "    model_name = \"BlueOrangeDigital/distilbert-cross-segment-document-chunking\"\n",
        "\n",
        "    id2label = {0: \"SAME\", 1: \"DIFFERENT\"}\n",
        "    label2id = {\"SAME\": 0, \"DIFFERENT\": 1}\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "      model_name,\n",
        "      num_labels=2,\n",
        "      id2label=id2label,\n",
        "      label2id=label2id\n",
        "    )\n",
        "\n",
        "    pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
        "\n",
        "    predictions = pipe(pairs)\n",
        "\n",
        "\n",
        "    n = len(ordered_sentences)\n",
        "    chunks_breaks = [\n",
        "      i+1\n",
        "      for i, pred in enumerate(predictions)\n",
        "      if pred[\"label\"] != \"SAME\"\n",
        "    ]\n",
        "\n",
        "    chunks = [\n",
        "      \"\\n\".join(ordered_sentences[i:j])\n",
        "      for i, j in zip([0] + chunks_breaks, chunks_breaks + [n])\n",
        "    ]\n",
        "    print(f\"Document split into {len(chunks)} chunks.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 448,
      "metadata": {
        "id": "KOWP-YScxr6f"
      },
      "outputs": [],
      "source": [
        "docu=[]\n",
        "for chunk in chunks:\n",
        "  if chunk is None:\n",
        "    continue\n",
        "  elif chunk == \"\":\n",
        "    continue\n",
        "  docu.append(chunk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 449,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YOFCuXwov4ct",
        "outputId": "71ae241f-215e-4e15-c166-0a7e19b37633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 23 chunks in FAISS index.\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.embeddings import GPT4AllEmbeddings  # Ensure the import is correct\n",
        "\n",
        "# Initialize the GPT4All embeddings model\n",
        "embedder = GPT4AllEmbeddings()\n",
        "\n",
        "# Initialize FAISS index with the correct embedding dimension\n",
        "embedding_dim = 384  # Set this to 384, based on the output size of GPT4All\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Function to generate embeddings for each chunk and store in FAISS\n",
        "def store_chunks_in_faiss(chunks):\n",
        "    for chunk in chunks:\n",
        "        # Generate embeddings\n",
        "        embeddings = embedder.embed_documents([chunk])  # Ensure input is a list\n",
        "\n",
        "        # Assuming embeddings is a list of arrays, extract the first embedding\n",
        "        embedding = embeddings[0]  # Get the first embedding\n",
        "\n",
        "        # Convert the embedding to a NumPy array if it's not already\n",
        "        embedding_np = np.array(embedding, dtype=\"float32\")\n",
        "\n",
        "        # Ensure the embedding has the correct shape\n",
        "        if embedding_np.ndim == 1:  # If shape is (384,)\n",
        "            embedding_np = embedding_np.reshape(1, -1)  # Reshape to (1, 384)\n",
        "\n",
        "        # Ensure the embedding matches the expected dimension\n",
        "        if embedding_np.shape[1] != embedding_dim:\n",
        "            raise ValueError(f\"Embedding has incorrect dimensions: {embedding_np.shape[1]} (expected {embedding_dim})\")\n",
        "\n",
        "        # Add embedding to FAISS index\n",
        "        index.add(embedding_np)\n",
        "\n",
        "    print(f\"Stored {index.ntotal} chunks in FAISS index.\")\n",
        "\n",
        "# Example usage\n",
        "# Replace 'docu' with your actual list of document chunks\n",
        "  # Example chunks\n",
        "store_chunks_in_faiss(docu)\n",
        "\n",
        "# Optional: Save FAISS index\n",
        "faiss.write_index(index, \"faiss_index.index\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 450,
      "metadata": {
        "id": "TD7TVJ2U4wgT"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.embeddings import GPT4AllEmbeddings  # Ensure the import is correct\n",
        "\n",
        "# Initialize the GPT4All embeddings model (if not already initialized)\n",
        "embedder = GPT4AllEmbeddings()\n",
        "\n",
        "# Function to perform similarity search\n",
        "def similarity_search(query, k=5):\n",
        "    # Generate the embedding for the query\n",
        "    query_embedding = embedder.embed_documents([query])[0]  # Get the first embedding\n",
        "    query_embedding_np = np.array(query_embedding, dtype=\"float32\").reshape(1, -1)  # Reshape to (1, 384)\n",
        "\n",
        "    # Perform the similarity search\n",
        "    distances, indices = index.search(query_embedding_np, k)  # Get the top k most similar embeddings\n",
        "\n",
        "    # Return the results\n",
        "    return distances, indices\n",
        "\n",
        "# Example usage\n",
        "#query = \"Considering the financial performance outlined in the Bloomberg article and the strategic partnerships mentioned in the Reuters report on Poodle Holdings, which division, represented by its initial, is poised for the most significant expansion in the upcoming fiscal year?\"\n",
        "distances, indices = similarity_search(query, k=5)\n",
        "\n",
        "# Output the results\n",
        "# print(\"Distances:\", distances)\n",
        "# print(\"Indices:\", indices)\n",
        "\n",
        "# If you want to access the original documents associated with the indices\n",
        "original_documents = [docu[i] for i in indices[0]]  # Assuming `docu` holds your original chunks\n",
        "# print(\"Similar Documents:\")\n",
        "l=\"\"\n",
        "for doc in original_documents:\n",
        "    l+=doc+\"\\n\"\n",
        "\n",
        "if(len(l)>19000):\n",
        "  distances, indices = similarity_search(query, k=3)\n",
        "\n",
        "  # Output the results\n",
        "  # print(\"Distances:\", distances)\n",
        "  # print(\"Indices:\", indices)\n",
        "\n",
        "  # If you want to access the original documents associated with the indices\n",
        "  original_documents = [docu[i] for i in indices[0]]\n",
        "\n",
        "  l=\"\"\n",
        "  for doc in original_documents:\n",
        "    l+=doc+\"\\n\"\n",
        "# print(len(l))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 451,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "peDuZ_m0D15A",
        "outputId": "ef17c5d4-3523-4011-d5ec-78e3b1267ac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yes\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoModelForSeq2SeqLM\n",
        "\n",
        "# Set up the Hugging Face model\n",
        "\n",
        "\n",
        "# Convert the documents to a single string\n",
        "joined_documents = \"\".join(l)\n",
        "\n",
        "# Build the prompt based on the classified query type\n",
        "if label == \"inference_query\":\n",
        "\n",
        "\n",
        "  pipe = pipeline(\"text2text-generation\", model=\"google/t5-v1_1-base\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"google/t5-v1_1-base\")\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-v1_1-base\")\n",
        "  prompt_content = (\n",
        "      \"Based on the following documents, please provide a one to two word answer. \"\n",
        "      \"Give a straight answer. If there is not enough information, return 'Insufficient Information'.\\n\\n\"\n",
        "      f\"Documents:\\n{joined_documents}\\n\\n\"\n",
        "      f\"Query: {query}\\nAnswer:\"\n",
        "  )\n",
        "  inputs = tokenizer(prompt_content, return_tensors=\"pt\")\n",
        "\n",
        "  # Generate the output\n",
        "  outputs = model.generate(inputs[\"input_ids\"], max_length=10, num_beams=5, early_stopping=True)\n",
        "\n",
        "  # Decode the output to text\n",
        "  answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  # Print or return the answer\n",
        "  # print(answer)\n",
        "elif label==\"temporal_query\":\n",
        "    prompt_content = (\n",
        "        f\"Based on the following documents, answer 'No' or 'Yes' to the query. Make sure to be accurate and return 'No' when the evidence is insufficient or the staement in the query is incorrect. Don't include any facts, just the final answer.\\n\\n\"\n",
        "        f\"Documents:\\n{joined_documents}\\n\\n\"\n",
        "        f\"Query: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "    candidate_labels = [\"yes\",\"no\"]\n",
        "\n",
        "    result = classifier(prompt_content, candidate_labels)\n",
        "\n",
        "    # Output the label with the highest score\n",
        "    first_label = result['labels'][0]  # Access the first label\n",
        "    answer=first_label\n",
        "else :\n",
        "    prompt_content = (\n",
        "      f\"Based on the following documents, answer 'YES' or 'NO' to the query. Make sure to be accurate .Return 'No' when the given comparsion is false. If the given comparison is true return 'Yes'. Don't include any facts, just the final answer.\\n\\n\"\n",
        "      f\"Documents:\\n{joined_documents}\\n\\n\"\n",
        "      f\"Query: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "    candidate_labels = [\"yes\",\"no\"]\n",
        "\n",
        "    result = classifier(prompt_content, candidate_labels)\n",
        "\n",
        "    # Output the label with the highest score\n",
        "    first_label = result['labels'][0]  # Access the first label\n",
        "    answer=first_label\n",
        "# print(response)\n",
        "\n",
        "def extract_first_answer(text):\n",
        "    # Find the index of \"Answer:\" and get the substring after it until the newline\n",
        "    answer_index = text.find(\"Answer:\")\n",
        "    if answer_index != -1:\n",
        "        # Find the index of the newline character after \"Answer:\"\n",
        "        newline_index = text.find(\"\\n\", answer_index)\n",
        "        # If there is no newline, get the rest of the string\n",
        "        if newline_index == -1:\n",
        "            return text[answer_index + len(\"Answer:\"):].strip()\n",
        "        else:\n",
        "            return text[answer_index + len(\"Answer:\"):newline_index].strip()\n",
        "    return None\n",
        "\n",
        "# answer_index = response.find(\"Answer:\")\n",
        "# answer = extract_first_answer(response)\n",
        "print(answer)  # Output: This is the answer you are looking for.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "metadata": {
        "collapsed": true,
        "id": "pg3Fmab1UMaD"
      },
      "outputs": [],
      "source": [
        "def extract_facts(text):\n",
        "    # Initialize an empty list to store facts\n",
        "    facts_list = []\n",
        "\n",
        "    # Split the text into lines\n",
        "    lines = text.strip().split('\\n')\n",
        "\n",
        "    # Flag to indicate if we are in the facts section\n",
        "    facts_section = False\n",
        "\n",
        "    for line in lines:\n",
        "        # Check if we are starting the facts section\n",
        "        if line.startswith(\"Facts:\"):\n",
        "            facts_section = True\n",
        "            continue  # Skip the \"Facts:\" line\n",
        "\n",
        "        # If we are in the facts section, add non-empty lines to the list\n",
        "        if facts_section and line.strip():\n",
        "            # Strip leading/trailing whitespace and add to the facts list\n",
        "            facts_list.append(line.strip().lstrip('-').strip())\n",
        "\n",
        "    return facts_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "facts = extract_facts(joined_documents)\n",
        "\n",
        "\n",
        "\n",
        "lists=[]\n",
        "for i in d:\n",
        "  x=i.metadata\n",
        "  for fact in facts:\n",
        "    if fact in i.page_content:\n",
        "      x['facts']=fact\n",
        "  lists.append(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 453,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLXTd1yDQ0vx",
        "outputId": "0a40f180-e93d-4ab6-cfd1-af2ddd9a9252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"query\": \"After the TechCrunch report on October 7, 2023, concerning Dave Clark's comments on Flexport, and the subsequent TechCrunch article on October 30, 2023, regarding Ryan Petersen's actions at Flexport, was there a change in the nature of the events reported?\",\n",
            "    \"answer\": \"yes\",\n",
            "    \"question_type\": \"temporal_query\",\n",
            "    \"evidence_list\": [\n",
            "        {\n",
            "            \"author\": \"none\",\n",
            "            \"category\": \"business\",\n",
            "            \"published_at\": \"2023-10-02T17:46:00+00:00\",\n",
            "            \"source\": \"Cnbc | World Business News Leader\",\n",
            "            \"title\": \"The inside story of Dave Clark's tumultuous last days at Flexport\",\n",
            "            \"url\": \"https://www.cnbc.com/2023/10/02/the-inside-story-of-dave-clarks-tumultuous-last-days-at-flexport.html\"\n",
            "        },\n",
            "        {\n",
            "            \"author\": \"Rebecca Bellan\",\n",
            "            \"category\": \"technology\",\n",
            "            \"published_at\": \"2023-10-22T19:15:29+00:00\",\n",
            "            \"source\": \"TechCrunch\",\n",
            "            \"title\": \"Tesla \\u2018digs its own grave with the Cybertruck,\\u2019 Convoy collapses and Rivian scores a win at Rebelle\",\n",
            "            \"url\": \"https://techcrunch.com/2023/10/22/tesla-digs-its-own-grave-with-the-cybertruck-convoy-collapses-and-rivian-scores-a-win-at-rebelle/\"\n",
            "        },\n",
            "        {\n",
            "            \"author\": \"Kyle Wiggers\",\n",
            "            \"category\": \"technology\",\n",
            "            \"published_at\": \"2023-10-07T20:15:26+00:00\",\n",
            "            \"source\": \"TechCrunch\",\n",
            "            \"title\": \"Sam Altman backs teens\\u2019 startup, Google unveils the Pixel 8 and TikTok tests an ad-free tier\",\n",
            "            \"url\": \"https://techcrunch.com/2023/10/07/sam-altman-backs-a-teens-startup-google-unveils-the-pixel-8-and-tiktok-tests-an-ad-free-tier/\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Function to generate the desired JSON format\n",
        "def generate_json_output():\n",
        "    # Define the question type (assuming it's already set to some label)\n",
        "    question_type = label\n",
        "\n",
        "    # Check if the answer is \"Insufficient Information\"\n",
        "    if answer == \"Insufficient Information.\":\n",
        "        evidence_list = []  # Empty evidence list\n",
        "        question_type = \"null_query\"  # Change the question type to \"null_query\"\n",
        "    else:\n",
        "        evidence_list = lists  # Use the lists variable as evidence\n",
        "\n",
        "    # Create the final output dictionary\n",
        "    output = {\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"question_type\": question_type,\n",
        "        \"evidence_list\": evidence_list\n",
        "    }\n",
        "\n",
        "    # Convert to JSON string\n",
        "    json_output = json.dumps(output, indent=4)\n",
        "    return json_output\n",
        "\n",
        "# Generate and print the JSON output\n",
        "result = generate_json_output()\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 453,
      "metadata": {
        "id": "teyAO4Jmggfd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
